# -*- coding: utf-8 -*-
"""Real_or_Fake_news_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pRRUfuuUTWyNgKoEnPU-CYgEVXw6Hc9h
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

true = pd.read_csv('True.csv',on_bad_lines='skip',encoding='latin-1')
fake = pd.read_csv('Fake.csv',on_bad_lines='skip',encoding='latin-1')

true['verdict']='True'
fake['verdict']='Fake'

data=pd.concat([true,fake],ignore_index=True).reset_index(drop=True)

data.info()
data.isnull().sum()
data.describe()

data.head()

plt.figure(figsize=(8, 6))
sns.countplot(x='verdict', data=data)
plt.title('Distribution of True and Fake News')
plt.show()

data['text_length'] = data['text'].apply(len)

plt.figure(figsize=(8, 6))
sns.boxplot(x='verdict', y='text_length', data=data)
plt.title('Article Length by Verdict')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(y='subject', data=data, hue='verdict')
plt.title('Distribution of Subjects by Verdict')
plt.show()

from collections import Counter

def get_top_n_words(corpus, n=None):
  vec = Counter()
  for text in corpus:
    for word in text.split():
      vec[word] += 1
  return vec.most_common(n)

top_words_true = get_top_n_words(data[data['verdict'] == 'True']['text'], 20)
top_words_fake = get_top_n_words(data[data['verdict'] == 'Fake']['text'], 20)

print("Top 20 words in true news:")
print(top_words_true)
print("\nTop 20 words in fake news:")
print(top_words_fake)

import string
string.punctuation

#Tokenize the text
!pip install nltk
import nltk

nltk.download('punkt')
from nltk.tokenize  import word_tokenize
text_data = data['text'].iloc[0]
words = word_tokenize(text_data)
print(words)

print(text_data)

#converting all words to lower case
words2 = [i.lower() for i in words]

#print words2
print(words2)
print("before stopwords:", len(words2))

from nltk.corpus import stopwords

# Download the 'stopwords' dataset
nltk.download('stopwords')

# Now you can access the stopwords list
stopwords_list = stopwords.words('english')
stopwords_list

#remove stopwords
words3 = []
for word in words2:
  if word not in stopwords_list:
    words3.append(word)
words3

print("after stopwords:", len(words3))

len(words)

punctuations = "#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~....''``'s'!"
words_without_punctuations = []
for word in words3:
  if word not in punctuations:
    words_without_punctuations.append(word)
words_without_punctuations

print("removing the punctuations:", len(words_without_punctuations))

from nltk.stem import PorterStemmer
porter_stemmer = PorterStemmer()

stemmed_words = [porter_stemmer.stem(word) for word in words_without_punctuations]
print(len(stemmed_words))

from wordcloud import WordCloud
# Create the word_string variable
# Assuming stemmed_words is the list of words to be included in the wordcloud
word_string = " ".join(stemmed_words)

wordcloud = WordCloud(collocations = False).generate(word_string)

#constructing a wordcloud for above stemmed_words and while constructing the word cloud make sure remove duplicated
from wordcloud import WordCloud
import matplotlib.pyplot as plt

words4 = [word for word in stemmed_words if word not in ["republican","tax","budget",'spend', 'democrat', 'program', 'said', 'trump']]

len(words4)

from collections import Counter

word_counts = Counter(stemmed_words)
print (word_counts)

word_counts

# Create the word_string variable
# Assuming words4 is the list of words to be included in the wordcloud
word_string = " ".join(words3)

wordcloud = WordCloud(collocations = True).generate(word_string)

wordcloud = WordCloud(width = 800, height = 600, background_color = 'white').generate_from_frequencies(word_counts)

plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

#Model Building
data.head()

#Splitting
X = data['text']
Y = data['verdict']
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)
feature_names = tfidf_vectorizer.get_feature_names_out()

#MultinomialNB
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()

model.fit(X_train_tfidf, Y_train)

y_pred_train = model.predict(X_train_tfidf) # Predict on TF-IDF transformed training data
y_pred_test = model.predict(X_test_tfidf)

from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(Y_test, y_pred_test)
print("Accuracy:", accuracy)
print(classification_report(Y_test,y_pred_test))

#metrics
from sklearn.metrics import accuracy_score
ac1 = accuracy_score(Y_train,y_pred_train)
print("Training Accuracy score:", round(ac1,3))
ac2 = accuracy_score(Y_test,y_pred_test)
print('Testing accuracy score:',round(ac2,3))

doc = " ".join(words4)
doc

#CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
#Create a count Vectorizer
vect = CountVectorizer(ngram_range=(3,3))

counts = vect.fit_transform([doc])
counts

#Getting vocabulary of the vectorizer
vocab = vect.get_feature_names_out()
vocab

top_20_bigrams = counts.toarray().sum(axis=0).argsort()[-20:]
#create a bar graph
plt.figure(figsize=(15,7))
plt.bar(vocab[top_20_bigrams], counts.toarray()[0, top_20_bigrams])
plt.xticks(rotation = 90)
plt.xlabel("Trigrams")
plt.ylabel("Count")
plt.title("Top 20 Bigrams")
plt.show()

# prompt: sentiment analysis for the above data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import string
import nltk
from nltk.tokenize  import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer

# Assuming you have the 'data' DataFrame from the previous code

!pip install textblob

from textblob import TextBlob

def get_sentiment(text):
  analysis = TextBlob(text)
  return analysis.sentiment.polarity

data['sentiment_polarity'] = data['text'].apply(get_sentiment)

# Assuming text_data is a string containing multiple lines of text
lines = text_data.splitlines()

# Create a DataFrame with a single column named 'text' containing the lines
df_lines = pd.DataFrame({'text': lines})

# Now you have a DataFrame with each line as a separate row.
print(df_lines.head())

df_lines.shape

#remove empty rows from the data
data = data.dropna(subset=['text'])
print(data.shape)

df_lines.head()

df_lines.tail()

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')
#Instantiate the entiment analyzer
analyzer = SentimentIntensityAnalyzer()
#creating new column for the Sentiment
df_lines['Sentiment_0'] = df_lines['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])
#Classify each statement  positive or negative based on the sentiment score
# Changed df_lines['Sentiment_0'] to df['Sentiment_0']
df_lines['Sentiment'] = df_lines['Sentiment_0'].apply(lambda x: 'positive' if x > 0 else 'negative')
#Display the updated DataFrame
print(df_lines)

df_lines['Sentiment'].value_counts()

import string
df_lines['text'] = df_lines['text'].str.replace('[{}]'.format(string.punctuation),' ')
df_lines.head()

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

df_lines['text'] = df_lines['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))
df_lines.head()

#stopwords
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english')) # Get the list of English stopwords using stopwords.words('english')
lemmatizer = WordNetLemmatizer()
df_lines['text'] = df_lines['text'].apply(lambda x: " ".join([word for word in x.split() if word not in stop_words])) # Use stop_words (the set of stopwords) in the list comprehension
df_lines.head()

Y = df_lines['Sentiment']
X = df_lines['text']

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(X)

from sklearn.naive_bayes import MultinomialNB
model1 = MultinomialNB()
model1.fit(X,Y)

y_pred = model1.predict(X)
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y,y_pred)
print(f"Accuracy Score:{round(accuracy, 2)}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assuming Y is your true labels and y_pred are your predicted labels
accuracy = accuracy_score(Y, y_pred)
precision = precision_score(Y, y_pred, average='weighted')  # Use 'weighted' for multi-class
recall = recall_score(Y, y_pred, average='weighted')  # Use 'weighted' for multi-class
f1 = f1_score(Y, y_pred, average='weighted')  # Use 'weighted' for multi-class

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-score: {f1:.2f}")

mnb_model = MultinomialNB()
mnb_model.fit(X_train_tfidf, Y_train)
mnb_y_pred = mnb_model.predict(X_test_tfidf)
mnb_accuracy = accuracy_score(Y_test, mnb_y_pred)
print("MultinomialNB Accuracy:", mnb_accuracy)

# prompt: create simple random forest classifier and XGboost and ADA boost and compare their accuracies

from sklearn.ensemble import RandomForestClassifier

# Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_tfidf, Y_train)
rf_y_pred = rf_model.predict(X_test_tfidf)
rf_accuracy = accuracy_score(Y_test, rf_y_pred)
print("Random Forest Accuracy:", rf_accuracy)

#XGBoost Classifier
from xgboost import XGBClassifier
xgb_model = XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(X_train_tfidf, Y_train)  # Y_train now has numerical values
xgb_y_pred = xgb_model.predict(X_test_tfidf)
xgb_accuracy = accuracy_score(Y_test, xgb_y_pred)
print("XGBoost Accuracy:", xgb_accuracy)

#XGBoost Classifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
label_encoder = LabelEncoder()

# Fit the encoder to your training data and transform it
Y_train_encoded = label_encoder.fit_transform(Y_train)
Y_test_encoded = label_encoder.transform(Y_test) # Transform Y_test as well


xgb_model = XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(X_train_tfidf, Y_train_encoded)  # Use the encoded Y_train
xgb_y_pred = xgb_model.predict(X_test_tfidf)

# Inverse transform the predictions to get original labels if needed
xgb_y_pred_original = label_encoder.inverse_transform(xgb_y_pred)

xgb_accuracy = accuracy_score(Y_test_encoded, xgb_y_pred)
print("XGBoost Accuracy:", xgb_accuracy)

# AdaBoost Classifier
from sklearn.ensemble import AdaBoostClassifier
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)
ada_model.fit(X_train_tfidf, Y_train)
ada_y_pred = ada_model.predict(X_test_tfidf)
ada_accuracy = accuracy_score(Y_test, ada_y_pred)
print("AdaBoost Accuracy:", ada_accuracy)

from sklearn.linear_model import LogisticRegression

# Logistic Regression Classifier
lr_model = LogisticRegression(max_iter=1000, random_state=42)  # Increase max_iter if needed
lr_model.fit(X_train_tfidf, Y_train)
lr_y_pred = lr_model.predict(X_test_tfidf)
lr_accuracy = accuracy_score(Y_test, lr_y_pred)
print("Logistic Regression Accuracy:", lr_accuracy)

# MultinomialNB
mnb_model = MultinomialNB()
mnb_model.fit(X_train_tfidf, Y_train)
mnb_y_pred = mnb_model.predict(X_test_tfidf)
mnb_accuracy = accuracy_score(Y_test, mnb_y_pred)
print("MultinomialNB Accuracy:", mnb_accuracy)

# Compare accuracies
print("\nAccuracy Comparison:")
print("Random Forest:", rf_accuracy)
print("XGBoost:", xgb_accuracy)
print("AdaBoost:", ada_accuracy)
print("Logistic Regression Accuracy:", lr_accuracy)
print("MultinomialNB Accuracy:", mnb_accuracy)

"""Saving the trained model"""

import pickle

filename = 'trained_model.sav'
pickle.dump(mnb_model, open(filename, 'wb'))

